{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network from scratch\n",
    "\n",
    "So let's build a neural network from scratch. We'll use just basic python and NumPy library to achieve this.\n",
    "\n",
    "The first thing that we have to know is how a basic NN(Neural Network) looks like.\n",
    "\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/sCLdl.png\" width=\"600\">\n",
    "\n",
    "\n",
    "As you can see there is an input layer which get's its input values from a certain dataset.\n",
    "And then each of the inputs get multiplied by some weight and then a bias is added to it when they are fed into the next set of neurons(i.e. hidden layer).\n",
    "\n",
    "Next there are activation functions taking care of nonlinearity and interaction effects and then we give an output. Then the loss function tries to find how far off are the outputs from the actual outputs and then it uses some optimisation function to update the weights and biases to get better outputs.\n",
    "\n",
    "We will go step by step and form a full-fledged NN network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular output\n",
    "\n",
    "Firstly, let's imagine that there are 4 input features and only one output neuron. And each of those connections have a different weight associated with it. Also, there is a bias associated with the output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.55\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [.2, .3, -.5, .9]\n",
    "bias = 2\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that each of the element in the input vector goes through an element wise multiplication with the weights vector. And finally we add the bias associated with that particular output neuron and print it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple outputs\n",
    "\n",
    "As you can see from the above image that if there is only one output then we don't create enough complexity in the network to actually map the whole picture. So, we need to have hidden layers in between the output and input layers which will certainly have multiple neurons (i.e. multiple outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights0 = [.2, .3, -.5, .9]\n",
    "weights1 = [.24, .43, -9.54, .19]\n",
    "weights2 = [.27, 7.3, -4.5, 4.9]\n",
    "\n",
    "bias0 = 2\n",
    "bias1 = 3\n",
    "bias2 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have here 4 inputs, and then 3 sets of weights as there are 3 outputs(say). Then we have 3 biases each connected to a particular neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "3.55, -24.04, 14.12\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "output0 = inputs[0]*weights0[0] + inputs[1]*weights0[1] + inputs[2]*weights0[2] + inputs[3]*weights0[3] + bias0 \n",
    "output1 = inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1 \n",
    "output2 = inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2 \n",
    "print (\"{:.2f}, {:.2f}, {:.2f}\".format(output0, output1, output2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have done earlier we do element wise multiplication; add them and then we add bias to it at the end. So we get all the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing it a smarter way\n",
    "\n",
    "In the previous example we did the same element wise multiplication, then addition and finally added the bias to it. It's too much of code. So, let's do it with the help of iteration using `for` loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [[.2, .3, -.5, .9],\n",
    "           [.24, .43, -9.54, .19],\n",
    "           [.27, 7.3, -4.5, 4.9]]\n",
    "\n",
    "biases = [2, 3, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = []\n",
    "\n",
    "for weight,bias in zip(weights, biases):\n",
    "  output = 0\n",
    "  for k,l in zip(inputs, weight):\n",
    "    output += k*l\n",
    "  output += bias\n",
    "  output_layer.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.55, -24.04, 14.12\n"
     ]
    }
   ],
   "source": [
    "print (\"{:.2f}, {:.2f}, {:.2f}\".format(output_layer[0], output_layer[1], output_layer[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically we have done the same using `for` loop and then printing it out. I kept the inputs, weights and biases same so that the answer can be verified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Dot Product\n",
    "\n",
    "We have done element wise multiplication followed by addition in the previous cells. In vector terms that is basically a [**dot product**](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products/v/vector-dot-product-and-vector-length). \n",
    "\n",
    "We can use python NumPy library to do quick dot product as you can see I have done below. And again the answer's the same which means it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.55 , -24.045,  14.12 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "output = np.dot(weights,inputs) + biases\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple inputs\n",
    "\n",
    "Now it's for sure that can't be just one row of input for any dataset. So we take multiple rows of data as input. And then we obtain the dot product using `np.dot` function and passing in `input_mul` and `weights`. \n",
    "\n",
    "But there is a trick here. Since the shape of `input_mul` is (3,4) and the shape of `weights` is also (3,4) so the dot product is not possible. We need to transpose the `weights` matrix so that the shape is changed to (4,3). Now we will be able to perform the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.55  , -24.045 ,  14.12  ],\n",
       "       [ -0.165 , -27.7252,  24.247 ],\n",
       "       [  7.56  ,   0.507 ,  35.138 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_mul = [[1,2,3,2.5],\n",
    "           [-2.4,7,3.43,-2.3],\n",
    "           [-4.6,.2,.3,7.3]]\n",
    "\n",
    "output = np.dot(input_mul, np.array(weights).T) + biases\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make this output layer a hidden layer and add another layer that will be the final layer. We add `weights2` and `biases2`. Do the same dot product twice and then we get the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -96.0435  ,   70.74215 , -142.237   ],\n",
       "       [-106.06186 ,   92.741766, -216.88805 ],\n",
       "       [  19.2371  ,  140.06421 , -146.1805  ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights2 = [[-.2, 4.3, .5],\n",
    "           [3.2, -.43, 3.19],\n",
    "           [3.27, 3.5, -4.9]]\n",
    "\n",
    "biases2 = [1, 4, -0.5]\n",
    "\n",
    "output_layer1 = np.dot(input_mul, np.array(weights).T) + biases\n",
    "output_layer2 = np.dot(output_layer1, np.array(weights2).T) + biases2\n",
    "output_layer2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thus we have finally created the structure of a NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding ReLU\n",
    "\n",
    "We will finally pass the outputs of the neurons through an activation function and then finally get an output in the 2nd layer of outputs.\n",
    "\n",
    "The `Layers` class has the weights, biases and also the `forward` method where we get the dot product added to the biases.\n",
    "We also have the `ActReLU` method that consists the activation function ReLU. \n",
    "\n",
    "We take the inputs(X), pass it through `Layers.forward()`. Then pass the outputs through activation function and then pass those outputs through `Layers.forward()` to get the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.72274344,  4.13052163],\n",
       "       [ 0.07976996, -0.3646472 ],\n",
       "       [ 1.32626998, 13.42650492]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[1.9,-22,-41.3,120.9],\n",
    "     [-0.3,20.4,12.5,-0.4],\n",
    "     [8.33,50.5,1.3,300.6]]\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "\n",
    "class Layers:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.arange(n_neurons).reshape(1, n_neurons)\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class ActReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "layer1 = Layers(4,16)\n",
    "activation1 = ActReLU()\n",
    "layer2 = LayerDense(16,2)\n",
    "\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "layer2.forward(activation1.output)\n",
    "layer2.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
